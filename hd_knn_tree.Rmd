---
title: "hd_knn_tree"
author: "Dany Park"
date: "01/03/2021"
output: 
  github_document:
    pandoc_args: --webtex
  
---
# K-nearest neighbor and Decision Tree
This project is to apply K-Nearest Neighbor and Decision Tree to the heart disease dataset and apply fitted model to predict the potential patient. Also, the models are compared with the Logistic Regression for their accuracy and predictability.

The [hd_log_reg](https://github.com/danypark91/hd_log_reg/blob/main/hd_log_reg_rmarkdown.md) already consists extensive explanation of the dataset. The project includes data visualization of the same dataframe. It will walk through the step-by-step procedure of the regression analysis and the performance of the predicted model.

## 1. K-nearest neighbor
### 1-1. Overview of KNN

K-nearest neighbors(KNN) is considered to be one of the most simplest and well-known non-parametric methods. The algorithm does not assume the parametric form, which allows more flexible approach to perform analysis. The classification method indentifies the given K points that are closest to the training independent variable x0. Then the conditional probability for a particular class is estimated. The largest probability is used for KNN to apply Bayes Rule and classify the test observations.

![knn_example](https://i.imgur.com/XXWScgF.png)

For example, assume that the K is chosen to be 3. Within the distanced boundary, two blue points are captured along with the orange point. The estimate probability for the blue equals to 2/3 and orange to 1/3. Then the algorithm predicts the boundary's class as blue. The right-hand side presents the decision boundary of all possible values of x0 with applying KNN algorithm and k=3. 


### 1-2. Importation and Alteration of Data
Before proceeding straight into the algorithm, I imported the project's dataframe. Like the previous logistic regression project, the erronous attribute name was corrected. However this time, the `knn` function required the only response variable as a factor(categorical variable).

```{r Data Importation, echo=FALSE}
#Import Dataset from the local device
df <- read.csv("Heart.csv", header = TRUE)

#change erronous attribute name: ï..age
colnames(df)[colnames(df)=='ï..age'] <- 'age'

#Check the type and convert the dependent variable into factors
df$target <- as.factor(df$target)
```
Also, prior to the analysis, normalization of dependent variable was conducted to equalize the weight and range. The `normalize` function helped to acquire the condition. The normalized dataset was divided into two sets: `train_df` was used to apply and train the `knn` alogorithm and the measure of predictability utlized the `test_df`.

```{r Normaliztion, echo=FALSE}
#Normalization function
normalize <- function(x){
  return ((x - min(x))/(max(x) - min(x)))
}

norm_df <- as.data.frame(lapply(df[,1:13], normalize))
head(norm_df,5)

#Combine the normalized dataframe with the target variable
norm_df <- cbind(norm_df, df$target)
colnames(norm_df)[colnames(norm_df)=="df$target"] <- "target"
head(norm_df,5)

#Split into Train and Test Datasets
library(caTools)
set.seed(1234)

sample = sample.split(norm_df, SplitRatio = 0.75)
train_df = subset(norm_df, sample==TRUE)
test_df = subset(norm_df,sample==FALSE)
```

### 1-3. Selection of K
As the algorithm is based on the distance based on the value of K, it is extremely important to choose appropriate value. The below image illustrates the difference betwen k=3 and k=5. The result of the choice between those values could significantly differ from one another. In order to choose the right K, `knn` should be performed multiple times and choose the K that has the least errors. 

![Difference](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/330px-KnnClassification.svg.png)

There are couple of points to consider:

* K is a positive integer
* K -> 1, less stable prediction
* As K increases, prediction becomes more stable. However, if the error increases, then rerun of `knn` could stop
* If tiebreaking within the range of K, then choose odd number

The below run is the sample run of KNN with K=15. The rate of error of prediction on the `test_df` is as low as 0.2209. The result can be considered as an accurate model. However, as stated above, `knn` should be performed multiple times with different K values to determine the best-fit model.

```{r Sample KNN run, echo=FALSE}
#K-Nearest Neighbor sample run, k=15
library(class)
knn_15 <- knn(train=train_df[1:13], test=test_df[1:13], cl=train_df$target, k=15)

#Predictability of the above model
table(knn_15, test_df$target)
print(paste("Error: ", round(mean(knn_15 != test_df$target),4))) #knn error rate
```

To minimize the effort, the list of error based on the value of k was created. For-loop command helped to populate the sequential list of error for K between 1 to 15. The graph represents the list, `knn-err`, and the value of K. As the trend suggests, error rate decreases significantly after k=6 and bounces back at 10. 8 and 9 are the most accurate models for the dataframe. However, as the tiebreaking rule suggests, 9 is chosen to proceed further steps for the analysis.

```{r For Loop KNN from 1 to 15, echo=FALSE}
#Error vs number of neighbors
knn_err <- list() #empty list

for (i in 1:15){
  #KNN
  temp <- mean((knn(train=train_df[1:13], test=test_df[1:13], cl=train_df$target, k=i)) != test_df$target)
  knn_err[[i]] <- temp
}

#Plot of K vs Error list
x <- seq(1, 15, by=1)
knn_errplot <- plot(x, knn_err, type="b", axes=TRUE,
                    xlab="K", ylab="Error Rate", main="K vs Error of KNN", col="Red")
```
The exact rate of error of the K=9 model is 0.1744 which lower than the k=15 model. Of course the accuracy of the model compare to the `test_df` reponse variable is 0.8256. 
```{r K=9, echo=FALSE}
#K=9, Fit KNN
df_knn_model <- knn(train=train_df[1:13], test=test_df[1:13], cl=train_df$target, k=9)
df_knn_model_acc <- mean(df_knn_model == test_df$target)
df_knn_model_err <- mean(df_knn_model != test_df$target)

print(paste("Accuracy of the Model : ", round(df_knn_model_acc,4)))
print(paste("Error of the Model : ", round(df_knn_model_err,4)))
```

### 1-4. Prediction and Performance Measure
As we dicovered the best-fit model of `KNN`, we should examine the model's predictability and its performance. The most commonly used technics are Confusion Matrix and Receiver Operating Characteristic Curve. A confusion matrix is a table used to exhibit the classification result on the test dataset. It contains two dimensions, ‘actual’ and ‘predicted’, and the cells are determined by the number of categories in a response variable. The below image explains the meaning of each cell and significant metrics.

![Confusion Matrix](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)

The confusion matrix states that the accuracy of the 9-nearest neighbor for the dataframe is 0.8256 with the 95% confidence interval of 0.7287 and 0.8990. The prediction result of the model is very promising in terms of the predictability.

The sensitivity is 0.8511, which means that out of 48 patients who suffered the heart disease, 40 patients were correctly diagnosed. The specificity score is 0.7949. Among the 39 patients who did not carry the heart disease, 31 patients were successfully categorized. The error of the model is 0.1744: 8 patients are categorized as Type I error where as 7 patients suffered Type II error. As the dataframe is related to health issues, Type II error could cause a devastating result. 

```{r KNN Confusion Matrix, echo=FALSE}
#Confusion Matrix of the KNN
library(caret)
df_knn_conf <- confusionMatrix(factor(df_knn_model), factor(test_df$target), positive=as.character(1))
df_knn_conf
```


![ROC Curve](https://ars.els-cdn.com/content/image/3-s2.0-B9780128030141000029-f02-01-9780128030141.jpg)

Another way to measure the predictability of the model is by deriving the ROC curve and AUC score. It is an excellent tool to graphically repsent the predictability of a binary classification. A ROC plots True Positive Rate vs False Positive Rate at different classification thresholds. Lower the classification threshold will result more items as positive. More the curve close to the blue line, more the accurate prediction is. The ROC curve below shows that it is close to the maximum plot that a ROC could be. 

Although ROC visualize the performance of the predicted model, it is very diffcult to quantify. AUC provides an aggregate measure of performance for all possible classification threshold. It measures the quality of the model's prediction irrespecitible to the chosen classification thresold. For KNN, AUC score is 0.9026 which is very close to 1.00. It is an evidence that the model's prediciton is statistically signifcant.

```{r KNN ROC and AUC, echo=FALSE, fig.height=5, fig.width=6}
library(kknn)
df_knn_model.alt <- train.kknn(as.factor(target)~., train_df, ks=9,  method="knn", scale=TRUE)
df_knn_model_fit <- predict(df_knn_model.alt, test_df, type="prob")[,2]

#ROC and AUC of the plot
library(ROCR)
df_knn_prediction <- prediction(df_knn_model_fit, test_df$target)
df_knn_performance <- performance(df_knn_prediction, measure = "tpr", x.measure = "fpr")
df_knn_roc <- plot(df_knn_performance, col="Red",
                   main="ROC Curve - 9NN",
                   xlab="False Positive Rate",
                   ylab="True Positive Rate")+
  abline(a=0, b=1, col="Grey", lty=2)+
  abline(v=0, h=1, col="Blue", lty=3)+
  plot(df_knn_performance, col="Red",add=TRUE)

df_knn_auc <- performance(df_knn_prediction, measure = "auc")
df_knn_auc <- df_knn_auc@y.values[[1]]
df_knn_auc
```

## 2. Decision Tree
Decision Tree is a tree structured regression model or classification method. It consists a root node which initiate the process and a leaf node which is the final result. The core process that builds the tree is called Binary Recursive Partitioning. It recursively splits the data into partitions (sub-populations). The process terminates after a particular stopping attribute is reached.

![Tree Example](https://www.researchgate.net/profile/Richard-Berk-2/publication/255584665/figure/fig1/AS:670716638789635@1536922716680/Recursive-Partitioning-Logic-in-Classification-and-Regression-Trees-CART.png)

The first step is to import the data and cleanse it like the previous classification methods. However, unlike the K-nearest neighbor, categorical variables should be converted into the type factor. After the dataframe's attributes get corrected, as usual, I splitted the data into the train and the test sets.
```{r Prepare the Dataframe, echo=FALSE}
#Convert categorical variable from int to factor
df <- read.csv("Heart.csv", header = TRUE)
colnames(df)[colnames(df)=='ï..age'] <- 'age'

df$sex <- as.factor(df$sex)
df$cp <- as.factor(df$cp)
df$fbs <- as.factor(df$fbs)
df$restecg <- as.factor(df$restecg)
df$exang <- as.factor(df$exang)
df$slope <- as.factor(df$slope)
df$thal <- as.factor(df$thal)

str(df)

#Split into Train and Test Datasets
library(caTools)
set.seed(1234)

sample_dt = sample.split(df, SplitRatio = 0.75)
train_dt_df = subset(df, sample_dt==TRUE)
test_dt_df = subset(df,sample_dt==FALSE)
```

### 2-1. Decision Tree Fitting
The Decision tree algorithm is applied to `train_dt_df`. Unlike the regression method of Decision Tree, Residual Sum of Squares(RSS) cannot be computed. Instead, either the Gini Index or the cross-entropy are typically used to evaludate the quality of the split. However, classfication error rate should be preferred when we assess the predictability of the model.

The * mark indicates the terminal nodes of  the model. Espically the table printed at the bottom shows the statistical result of the model. The table starts from the smallest tree (no splits) to the largest (23 splits). To easily identify the best split, we should focus on `xerror` column of the CP table. However, please keep in mind that the error has been scaled to the first node with an error of 1. The graph clearly indicates that the best decision tree has 13 splits (12 terminal nodes).
```{r Fitting the model, echo=FALSE}
#Decision Tree for the train model
library(rpart)

df_dt_model <- rpart(as.factor(target)~., data=train_dt_df, method = "class", 
                     control=rpart.control(xval=10, minbucket=2,cp=0)) #response variable = factor
df_dt_model
printcp(df_dt_model) # display the results
plotcp(df_dt_model,col="Red") # visualize cross-validation results

```

### 2-2. Prune
The subtree of 13 splits has been applied to the pruning process based on the optimal CP value. The visualization of the pruned decision tree helps us to easily understand the model.

Interpreting the left most model:

* The root node starts with the ratio of patients suffering heart disease. 54% of the patients suffered the disease
* The next node asks the patient's `thal`: inherited blood disorder. If the patient condition is either excess(0), normal(1) or reversable defect(3), then go down to the left child node. 45% who had listed condition suffered heart disease at the rate of 27%.
* The next node asks the patient's `ca`: number of major vessels. If the patient has or has more than 1 vessel, proceeds further to the left child node. 
* The final probability that the patient having the heart disease is 0.04


```{r Prune, echo=FALSE,message=FALSE}
library(rpart.plot)
#Prune the tree based on the result
df_dt_prune <- prune(df_dt_model, cp=0.0101010)
                       #df_dt_model$cptable[which.min(df_dt_model$cptable[,"xerror"]),"CP"])
rpart.plot(df_dt_prune)
```

### 2-3. Prediction and Performance Measure
Similar to the Logistic Regression and the K-nearest neighbor, confusion matrix and ROC curve are used to measure the predictability of the model. 
The overall accuracy of the model is 0.7558 with the 95% confidence interval of 0.6513 and 0.8420. The algorithm correctly predicted 65 patients out of 86 patients. out of 39 patients who did not carry the heart disease, 10 patients were falsely diagnosed (Type I error): carrying heart disease. Similary, `test_dt_df` categorized 47 heart disease patients, 11 patients were predicted not suffering heart disease (Type II error). 

```{r DT Confusion Matrix, echo=FALSE}
#ConfusionMatrix
df_dt_model_fit <- predict(df_dt_prune, newdata=test_dt_df, type="prob")[,2]
df_dt_model_conf <- ifelse(df_dt_model_fit>0.5,1,0)

df_dt_conf <- confusionMatrix(as.factor(df_dt_model_conf), as.factor(test_dt_df$target), positive=as.character(1))
df_dt_conf
```

The ROC curve is above the grey line. It indicates that the prediction at any given point is correctly classifying the result. The quantifying measure of the curve is 0.7559 which can be considered as moderately accurate model.
```{r DT ROC and AUC, echo=FALSE, fig.height=5, fig.width=6}
#ROC Curve and AUC Score
df_dt_prediction <- prediction(df_dt_model_fit, as.factor(test_dt_df$target))
df_dt_performance <- performance(df_dt_prediction, measure = "tpr", x.measure = "fpr")
df_dt_roc <- plot(df_dt_performance, col="Red",
                  main="ROC Curve - Decision Tree",
                  xlab="False Positive Rate",
                  ylab="True Positive Rate")+
  abline(a=0, b=1, col="Grey", lty=2)+
  abline(v=0, h=1, col="Blue", lty=3)+
  plot(df_dt_performance, col="Red",add=TRUE)

df_dt_auc <- performance(df_dt_prediction, measure="auc")
df_dt_auc <- df_dt_auc@y.values[[1]]
df_dt_auc

```


## 3. Comparison with other Model
The Previous chapters and other project applied various classification algorithms. All those algorithms have advantages over one another. For the Heart Disease dataframe, Logistic regression, K-nearest neighbor and the Decision Tree are chosen. Each classification has advantages to one another. This chapter will compare the result of those methods and distinguish which model is better to predict the potential patient.

### 3-1. Logistic Regression
Detailed study of logistic regression of the heart disease dataframe can be found in [hd_log_reg](https://github.com/danypark91/hd_log_reg). The repository consists extensive step-by-step data analysis using logistic regression. The underlying statistical logic, data visualization and the prediciton results are well explained. The below code is directly extracted from the repository to compare the result with K-nearest neighbor and Decision Tree Classification. 
```{r Logistic Regression, echo=TRUE, fig.height=5, fig.width=6}
#Logistic Regression Model
library(MASS)
df_model.part <- glm(target~sex+cp+trestbps+thalach+oldpeak+ca, data=train_dt_df, family=binomial(link="logit"))
df_model_fit <- predict(df_model.part, newdata=test_dt_df, type="response")
df_model_confmat <- ifelse(df_model_fit >0.5, 1, 0)

df_log_conf <- confusionMatrix(factor(df_model_confmat), factor(test_dt_df$target), positive=as.character(1))
df_log_conf

df_prediction <- prediction(df_model_fit, test_dt_df$target)
df_performance <- performance(df_prediction, measure = "tpr", x.measure="fpr")

plot(df_performance, col = "Red", 
     main = "ROC Curve - Logistic Regression",
     xlab="False Postiive Rate", ylab="True Positive Rate")+
  abline(a=0, b=1, col= "Grey", lty=2)+
  abline(v=0, h=1, col= "Blue", lty=3)+
  plot(df_performance, col = "Red", 
       main = "ROC Curve - Logistic Regression",
       xlab="False Postiive Rate", ylab="True Positive Rate",add=TRUE)

df_auc <- performance(df_prediction, measure = "auc")
df_auc <- df_auc@y.values[[1]]
print(paste("AUC Score: ", lapply(df_auc,round,4)))
```

### 3-2. Metrics Dataframe
To vizualize the important metrics, important figures are gathered to create a separate dataframe. In this case, it will be easier to visualize the comparison using `ggplot`. The gathered metrics are accuracy with its confidence intervals, sensitivity, specificity, f1 score and auc score. 
```{r Summary Dataframe, echo=FALSE}
#Model Fit summary dataframe
df_ci <- data.frame(type=c("Logistic Regression", "9-Nearest Neighbor", "Decision Tree"), 
                    acc=c(df_log_conf$overall[1], df_knn_conf$overall[1], df_dt_conf$overall[1]),
                    lowci=c(df_log_conf$overall[3], df_knn_conf$overall[3], df_dt_conf$overall[3]),
                    upci=c(df_log_conf$overall[4], df_knn_conf$overall[4], df_dt_conf$overall[4]),
                    sens=c(df_log_conf$byClass[1], df_knn_conf$byClass[1], df_dt_conf$byClass[1]),
                    spec=c(df_log_conf$byClass[2], df_knn_conf$byClass[2], df_dt_conf$byClass[2]),
                    f1=c(df_log_conf$byClass[7], df_knn_conf$byClass[7], df_dt_conf$byClass[7]),
                    auc =c(df_auc, df_knn_auc, df_dt_auc))
head(df_ci,3)
```

### 3-3. Graphical Presentation

- Accuracy

The first comparison is the accuracy of the prediciton. Accuracy is the number of all correct predictions over the total test datasets. The maximum value that can be achieved is 1.0 whereas the minimum is 0.0. Of course, higher score indicates the better predictability.

$$ Accuracy = \frac{(TP + TN)}{(TP + TN + FP + FN)} $$

where:

* TP = True Positive
* TN = True Negative
* FP = False Positive
* FN = False Negative

The graph shows that the accuracy of the 9-nearest neighbor classification is the best for the classification. 82.56% of the patients were classified correctly if KNN algorithm was applied. Followed by the logistic regression, 77.91% and the decision tree, 76.74%. This result indicates that applying KNN algorithm to the dataframe has the highest likelihood of correctly predicting the heart disease. 

```{r Accuracy Comparison, echo=FALSE}
#Accuracy Comparison with 95% Confidence Interval
library(tidyverse)
library(ggsci)
ggplot(data=df_ci, aes(type,acc))+
  labs(title="Comparison of Classification", subtitle="Accuracy and Confidence Interval", x="Classification", y="Accuracy")+
  geom_point(size=5, aes(color=type))+
  geom_text(aes(label=round(acc,4), hjust=-0.3, vjust=0.3))+
  geom_errorbar(aes(ymax=upci, ymin=lowci),width=0.2)+
  theme_bw()+
  scale_fill_npg()

```

- Sensitivity

Sensitivity is the number of correct positive predictions by the total number of positives. It is also called as recall or true positive rate(TPR). Similar to the accuracy, the maximum possible score is 1.0 and the minimum is 0.0. For this dataset, we can interpret that its the ability to detect positive cases on patients who do have the heart disease. High sensitivity could mean that the chances for misdiagnosing, not carrying the disease, is low. In addition, this is the key metrics in health dataset as it is directly related to the type II error. In any prediction related to the patient's health, type II error could have tremendous impact.

$$ Sensitivity = \frac{TP}{(TP + FN)} $$

where:

* TP = True Positive
* FN = False Negative

The 9-nearest neighbor classfication has the highest sensitivity score of 85.11%. The decision tree followed with 80.85% and finally the logistic regression with 76.60%. This means when a set of positive results comes in, KNN is least likely to falsely diagnose patients having bogus heart diease.

```{r Sensitivity, echo=FALSE}
#Sensitivity
ggplot(data=df_ci, aes(type, sens))+
  geom_point(size=5, aes(color=type))+
  geom_text(aes(label=round(sens,4), hjust=-0.3, vjust=0))+
  labs(title="Comparison of Classification", subtitle="Sensitivity", x="Classification Method", y="Sensitivity")+
  theme_bw()+
  scale_fill_npg()
```

- Specificity

Specificity is derived as  the number of true negative over the total number of negatives. It is also known as True Negative Rtae(TNR). Similar to the sensitivity, the best measure is 1.0 and the worst is 0.0. Specificity important to correctly reject the patients without the heart disease. As specificity includes Type I error, it may not have as devastating result as sensitivity.

$$Specificity = \frac{TN}{(TN + FP)}$$

where:

* TN = True Negative
* FP = False Positive

The 9-nearest neighbor and the logistic regression has the same specificity of 79.49%. It means that they have the same ability to detact the patient without the disease. Followed by the decision tree with the specificity of 71.79%.

```{r Specificity, echo=FALSE}
#Specificty
ggplot(data=df_ci, aes(type, spec))+
  geom_point(size=5, aes(color=type))+
  geom_text(aes(label=round(spec,4), hjust=-0.3, vjust=0))+
  labs(title="Comparison of Classification", subtitle="Specificity", x="Classification Method", y="Specificity")+
  theme_bw()+
  scale_fill_npg()
```

- F1 Score

F1 score is another way to statistically measure the accuracy of the classification. F1 score is the weighted average of the precision and recall. Recall is, as we already know, same as the Sensitivity. Precision measures the positivity of the positively predicted class. The best value is 1.0 and worst score of 0.0. In this dataset, F1 score would indicate the harmonic average between the positively predicted versus actual heart disease patients. 

![Insight](https://newbiettn.github.io/images/confusion-matrix-noted.jpg)

$$ F_1 = \frac{2}{recall^{-1} + precision^{-1}} = \frac{TP}{TP+\frac{1}{2}(FP + FN)}$$

The 9-nearest neighbor has the highest F1 score of 84.21%. Decision Tree and logistic regression has very similar F1 score with decision tree has slightly higher score of 79.17%. 

```{r F1, echo=FALSE}
#F1 Score
ggplot(data=df_ci, aes(type, f1))+
  geom_point(size=5, aes(color=type))+
  geom_text(aes(label=round(f1,4), hjust=-0.3, vjust=0))+
  labs(title="Comparison of Classification", subtitle="F1 Score", x="Classification Method", y="F1 Score")+
  theme_bw()+
  scale_fill_tron()
```

- ROC and AUC

The mixture of the ROC curve makes difficult to determine which classification method is better in predicting. The overlapping curves makes it difficult to rank. In such cases, comparing ROC Curve is not the efficient method. However, AUC quantifies the ROC curve and hence helps to easily compare between the models. The maximum AUC is 1.0 and the lowest of 0.0.
AUC of the 9-nearest neighbor is 0.9026 which is very close to 1.0. Logistic Regression ranked next with the AUC of 0.8696 and Decision Tree of 0.7608. 

```{r ROC Comparison, echo=FALSE, fig.height=5, fig.width=6}
#ROC Comparison
plot(df_performance, main="ROC Curve: Comparison", col="Red")+
  abline(a=0, b=1, col= "Grey", lty=2)+
  abline(v=0, h=1, col= "Blue", lty=3)
par(new=TRUE)
plot(df_knn_performance, col="Orange")
par(new=TRUE)
plot(df_dt_performance, col="Dark Red")
```

```{r AUC Score, echo=FALSE}
#AUC
ggplot(data=df_ci, aes(type, auc))+
  geom_point(size=5, aes(color=type))+
  geom_text(aes(label=round(auc,4), hjust=-0.3, vjust=0))+
  labs(title="Comparison of Classification", subtitle="Area Under Curve", x="Classification Method", y="AUC")+
  theme_bw()+
  scale_fill_npg()
```

We can conclude that for the heart disease dataset, 9-nearest neighbor classification has the highest metrics for the important scores that could be derived from the predicted model. Especially the accuracy and F1 score of the KNN mounts over the other classifier, which are directly related to the performance of the predicted model. Hence to successfully predict the potiential heart disease patients, it is highly recommended to use 9-nearest neighbor method. Of course, the series of the medical test should be carefully conducted as the model isn't able to perfectly detact the patients. 