---
title: "hd_knn_tree"
author: "Dany Park"
date: "01/03/2021"
output: rmarkdown::github_document
---
# K-Nearest Neighbor and Decision Tree
This project is to apply K-Nearest Neighbor and Decision Tree to the heart disease dataset and apply fitted model to predict the potential patient. Also, the models are compared with the Logistic Regression for their accuracy and predictability.

The [hd_log_reg](https://github.com/danypark91/hd_log_reg/blob/main/hd_log_reg_rmarkdown.md) already consists extensive explanation of the dataset. The project includes data visualization of the same dataframe. It will walk through the step-by-step procedure of the regression analysis and the performance of the predicted model.

## 1. K-Nearest Neighbor
### Overview of KNN



```{r Data Importation, echo=TRUE}
#Import Dataset from the local device
df <- read.csv("Heart.csv", header = TRUE)

#change erronous attribute name: ï..age
colnames(df)[colnames(df)=='ï..age'] <- 'age'

#Check the type and convert the dependent variable into factors
df$target <- as.factor(df$target)
```


```{r Normaliztion, echo=TRUE}
#Normalization function
normalize <- function(x){
  return ((x - min(x))/(max(x) - min(x)))
}
norm_df <- as.data.frame(lapply(df[,1:13], normalize))
head(norm_df,5)
```


```{r Combine and Train_test_split, echo=TRUE}
#Combine the normalized dataframe with the target variable
norm_df <- cbind(norm_df, df$target)
colnames(norm_df)[colnames(norm_df)=="df$target"] <- "target"
head(norm_df,5)

#Split into Train and Test Datasets
library(caTools)
set.seed(1234)

sample = sample.split(norm_df, SplitRatio = 0.75)
train_df = subset(norm_df, sample==TRUE)
test_df = subset(norm_df,sample==FALSE)
```

```{r Sample KNN run, echo=TRUE}
#K-Nearest Neighbor sample run, k=15
library(class)
knn_15 <- knn(train=train_df[1:13], test=test_df[1:13], cl=train_df$target, k=15)

#Predictability of the above model
table(knn_15, test_df$target)
mean(knn_15 != test_df$target) #knn error rate
```

```{r For Loop KNN from 1 to 15, echo=TRUE}
#Error vs number of neighbors
knn_err <- list() #empty list

for (i in 1:15){
  #KNN
  temp <- mean((knn(train=train_df[1:13], test=test_df[1:13], cl=train_df$target, k=i)) != test_df$target)
  knn_err[[i]] <- temp
}

#Plot of K vs Error list
x <- seq(1, 15, by=1)
knn_errplot <- plot(x, knn_err, type="b", axes=TRUE,
                    xlab="K", ylab="Error Rate", main="K vs Error of KNN", col="Red")
```

```{r K=9, echo=TRUE}
#K=9, Fit KNN
df_knn_model <- knn(train=train_df[1:13], test=test_df[1:13], cl=train_df$target, k=9)
df_knn_model_acc <- mean(df_knn_model == test_df$target)
df_knn_model_err <- mean(df_knn_model != test_df$target)

print(paste("Accuracy of the Model : ", round(df_knn_model_acc,4)))
print(paste("Error of the Model : ", round(df_knn_model_err,4)))
```

```{r Confusion Matrix, echo=TRUE}
#Confusion Matrix of the KNN
library(caret)
df_knn_conf <- confusionMatrix(factor(df_knn_model), factor(test_df$target), positive=as.character(1))
df_knn_conf
```

```{r ROC and AUC, echo=TRUE, fig.height=6, fig.width=6}
library(kknn)
df_knn_model.alt <- train.kknn(as.factor(target)~., train_df, ks=9,  method="knn", scale=TRUE)
df_knn_model_fit <- predict(df_knn_model.alt, test_df, type="prob")[,2]

#ROC and AUC of the plot
library(ROCR)
df_knn_prediction <- prediction(df_knn_model_fit, test_df$target)
df_knn_performance <- performance(df_knn_prediction, measure = "tpr", x.measure = "fpr")
df_knn_roc <- plot(df_knn_performance, col="Red",
                   main="ROC Curve - KNN",
                   xlab="False Positive Rate",
                   ylab="True Positive Rate")+
  abline(a=0, b=1, col="Grey", lty=2)+
  abline(v=0, h=1, col="Blue", lty=3)+
  plot(df_knn_performance, col="Red",add=TRUE)

df_knn_auc <- performance(df_knn_prediction, measure = "auc")
df_knn_auc <- df_knn_auc@y.values[[1]]
df_knn_auc
```
